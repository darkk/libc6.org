Title: Введение
Slug: intro
Category: Виртуализация
Date: 2012-04-26 14:26
Source: False

[TOC]

## Наш выбор

### Debian

[![Debian][3]][4]

Почему [Debian][4]? Эта операционная система мне близка и понятна, так что при
выборе дистрибутива мучений, терзаний и метаний испытано не было. Особых
преимуществ перед Red Hat Enterprise Linux у него нет, но было принято решение
работать со знакомой системой.

Если вы планируете самостоятельно развернуть инфраструктуру, используя
аналогичные технологии, я бы посоветовал взять именно RHEL: благодаря хорошей
документации и хорошо написаным прикладным программам это будет если не на
порядок, то уж точно раза в два проще, а благодаря развитой системе
сертификации без особого труда можно будет найти некоторое количество
специалистов, на должном уровне знакомых в данной ОС.

Мы же, повторюсь, решили использовать **Debian Squeeze** с набором пакетов из
**Sid/Experimental** и некоторыми пакетами, бэкпортированными и собранными с
нашими патчами.

В планах имеется публикация репозитория с пакетами.

### KVM

[![KVM][5]][2]

При выборе технологии виртуализации рассматривались два варианта — Xen и KVM.

Замечу, что лично я не очень хорошо знаю Xen, его архитектуру и уж тем более
мелкие особенности — в основном я знакомился с ним в качестве гостя. Нет
повода сказать, что Xen чем-то плох (потому, что он ещё не полностью вошёл в
ядро, или у него что-то не так с производительностью, или еще по какой-то
причине). Ничего определённого нельзя сказать и в плане производительности: в
каких-то тестах KVM на 10-20 процентов опережает Xen по всем показателям, а
где-то выигрывает Xen. Фактически, на текущий момент они практически
сравнялись по функционалу, производительности, надёжности. И в принципе, не за
горами тот день, когда Xen также войдёт в ядро. Уже вошёл в состав ядра
http://virtually-a-machine.blogspot.com/2011/06/xen-support-in-mainline-linux-
kernel.html.

Также во внимание принимался факт наличия огромного количества разработчиков,
хостеров, комерческих решений именно на базе Xen — тем интереснее было
провести в жизнь решение именно на базе KVM.

Основной же причиной, по которой мы решили использовать именно KVM, является
необходимость запуска виртуальных машин с FreeBSD и, в перспективе, MS
Windows.

### libvirt

![libvirt][6]

Для управления виртуальными машинами оказалось чрезвычайно удобно использовать
**libvirt** и продукты, использующие ее API: **virsh**, **virt-manager**,
**virt-install**, пр.

**libvirt** — это система, которая хранит настройки виртуальных машин, управляет ими, ведёт по ним статистику, следит за тем, чтобы при старте у виртуальной машины поднимался интерфейс, подключает устройства к машине — в общем, выполняет кучу полезной работы и еще немножко сверх того.

Разумеется, решение не идеально. Из минусов **libvirt** следует назвать:

  * Абсолютно невменяемые сообщения об ошибках.
  * Невозможность изменять часть конфигурации виртуальной машины на лету, хотя QMP ([QEMU Monitor Protocol][7]) это вполне позволяет.
  * Иногда к libvirtd по непонятной причине невозможно подключиться — он перестаёт реагировать на внешние события.

### cgroups

![cgroups][8]

Основной проблемой в реализации услуги в самом начале представлялось
лимитирование ресурсов для виртуальных машин. В Xen эта проблема была решена
при помощи внутреннего шедулера, распределяющего ресурсы между виртуальными
машинами — и что самое прекрасное, была реализована возможность лимитировать и
дисковые операции в том числе.

В KVM ничего такого не было до появления механизма распределения ресурсов ядра
[cgroups][9]. Как обычно в Linux, доступ к этим функциям был реализован
посредством специальной файловой системы **cgroup**, в которой при помощи
обычных системных вызовов write() можно было добавить процесс в группу,
назначить ему его вес в попугаях, указать ядро, на котором он будет работать,
указать пропускную способность диска, которую этот процесс может использовать,
или, опять же, назначить ему вес.

Профит в том, что всё это реализуется внутри ядра, и использовать это можно не
только для сервера, но и для десктопа (что и использовали в известном «[The
~200 Line Linux Kernel Patch That Does Wonders][10]»). И на мой взгляд, это
одно из самых значительных изменений в ветке 2.6, не считая [любимого
12309][11], а не запиливание очередной файловой системы. Ну, разве что, кроме
POHMELFS (но чисто из-за названия).

### libguestfs

[![libguestfs][12]][13]

Отношение к этой библиотеке-утилите у меня весьма неоднозначное.

С одной стороны это выглядит примерно так:

![libguestfs is a swiss knife][14]

И ещё эту штуку чертовски сложно собрать из исходников и тем более в пакет:
иногда мне кажется, что Linux From Scratch собрать с нуля несколько проще.

С другой стороны — очень мощная штука, которая позволяет создавать образы для
виртуальных машин, модифицировать их, ужимать, ставить grub, модифицировать
таблицу разделов, управлять конфигурационными файлами, переносить «железные»
машины в виртуальную среду, переносить виртуальные машины с одного образа на
другой, переносить виртуальные машины из образа на железо и, честно говоря,
тут меня фантазия немного подводит. Ах, да: ещё можно запустить демон внутри
виртуальной машины Linux и получить доступ к данным виртуальной машины вживую,
и всё это делать на shell, python, perl, java, ocaml. Это краткий и далеко не
полный список того, что можно сделать с **libguestfs**.

Интересно, что большая часть кода в **libguestfs** генерируется в момент
сборки, равно как и документация к проекту. Очень широко используется ocaml,
perl. Сам код пишется на C, который потом оборачивается в OCaml, и
повторяющиеся куски кода генерируются сами. Работа с образами осуществляется
путём запуска специального сервисного образа (supermin appliance), в который
через канал внутрь него отправляются команды. Внутри этого образа содержится
некоторый rescue набор утилит, таких как parted, mkfs и прочих полезных в
хозяйстве системного администратора.

Я с недавнего времени его даже дома стал использовать, когда выковыривал из
образа nandroid нужные мне данные. Но для этого требуется ядро с поддержкой
yaffs.

## Прочее

Ниже приведено ещё несколько интересных ссылок на описание использованных
пограммных средств — почитать и поизучать самостоятельно, если интересно.
Например, [про утилиту][15] для массовой работы с конфигурационными файлами,
[KVM best practices][16] от товарищей из IBM. Рекомендую!

  1. [Пост Daniel Berrange об использовании cgroups с KVM и LXC.][17]
  2. [Пост Richard Jones об использовании libguestfs для получения маленького размера "базового" образа виртуальной машины.][18]
  3. [BKL #12309 на bugzilla.kernel.org][19]

## В следующей части

[Следующая часть][20] будет про установку необходимых программ, и их базовую
настройку.

   [1]: http://habrahabr.ru/blogs/virtualization/117226/ 
   [2]: http://linux-kvm.org
   [3]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/debian.jpg
   [4]: http://www.debian.org
   [5]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/kvm.png
   [6]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/libvirt.png
   [7]: http://wiki.qemu.org/QMP
   [8]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/kernel.gif
   [9]: http://www.kernel.org/doc/Documentation/cgroups/cgroups.txt
   [10]: http://marc.info/?l=linux-kernel&m=128978361700898&w=2
   [11]: http://lurkmore.ru/12309
   [12]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/libguestfs.png
   [13]: http://libguestfs.org/
   [14]: http://sprinthost.ru/img/articles/20100602_Habrahabr_Intro/libguestfs_swiss.jpg
   [15]: http://augeas.net
   [16]: http://publib.boulder.ibm.com/infocenter/lnxinfo/v3r0m0/topic/liaat/liaatbestpractices_pdf.pdf
   [17]: http://berrange.com/posts/2009/12/03/using-cgroups-with-libvirt-and-lxckvm-guests-in-fedora-12/
   [18]: http://rwmj.wordpress.com/2010/10/30/notes-on-producing-a-minimal-compressed-filesystem/
   [19]: https://bugzilla.kernel.org/show_bug.cgi?id=12309
   [20]: http://habrahabr.ru/blogs/virtualization/120717/
